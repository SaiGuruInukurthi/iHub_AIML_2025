{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAKrrEmgLZ_J"
      },
      "source": [
        "# **Student Training Program on AIML**\n",
        "### MODULE: CLASSIFICATION-1\n",
        "### LAB-2 : Using KNN for Text Classification\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeRAsurrdLY"
      },
      "source": [
        "## **Section 1: Understanding NLP tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9N2m-HrgWY"
      },
      "source": [
        "In this lab we will be using KNN on a real world NLP application i.e. is text classification. But first look at some NLP techniques for text classification and tools that we use when we want to use python for NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMJhTF89MmT_"
      },
      "source": [
        "## Section 1.2: Data Cleaning and Preprocessing step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXVjJHk3urf"
      },
      "source": [
        "Raw text must be processed and converted into a form so that it is suitable to use with various machine-learning algorithms.  \n",
        "In case of text, there are lots of things that need to be taken into account.  \n",
        "\n",
        "\n",
        "1.   Removing numbers from the text\n",
        "2.   Handling capitalization and punctuation.\n",
        "3.   Stemming and Lemmatizing text.  \n",
        "\n",
        "And most importantly, one can't just use words or images directly in algorithms; they need to be converted into vectors- a form that algorithms can understand.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYq4np0xqUBr"
      },
      "source": [
        "### **NLTK**\n",
        "NLTK (or Natural Language Tool Kit) is a commonly used library for processing text. We will use this tool in this lab. Lets first install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFOZHWV3_ABt",
        "outputId": "60a58f22-eeac-41f5-ea46-50a189221f20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\IC1807\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gpgq3SQK5IOr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def cleanText(text, lemmatize, stemmer):\n",
        "    \"\"\"Method for cleaning text from train and test data. Removes numbers, punctuation, and capitalization. Stems or lemmatizes text.\"\"\"\n",
        "\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    if isinstance(text, numpy.int64):\n",
        "        text = str(text)\n",
        "    try:\n",
        "        text = text.decode()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "    soup = BeautifulSoup(text, \"lxml\")\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r\"[^A-Za-z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    if lemmatize:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        def get_tag(tag):\n",
        "            if tag.startswith('J'):\n",
        "                return wordnet.ADJ\n",
        "            elif tag.startswith('V'):\n",
        "                return wordnet.VERB\n",
        "            elif tag.startswith('N'):\n",
        "                return wordnet.NOUN\n",
        "            elif tag.startswith('R'):\n",
        "                return wordnet.ADV\n",
        "            else:\n",
        "                return ''\n",
        "\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)  # Generate list of tokens\n",
        "        tagged = pos_tag(tokens)\n",
        "        for t in tagged:\n",
        "            try:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0], get_tag(t[1][:2])))\n",
        "            except:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0]))\n",
        "        return text_result\n",
        "\n",
        "    if stemmer:\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)\n",
        "        snowball_stemmer = SnowballStemmer('english')\n",
        "        for t in tokens:\n",
        "            text_result.append(snowball_stemmer.stem(t))\n",
        "        return text_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuBxfzbuzrs1",
        "outputId": "ae28687d-6334-493d-e7de-8cddb92f89b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Troubling\n",
            "troubl\n",
            "trouble\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"Troubling\"\n",
        "sample_text_result = cleanText(sample_text, lemmatize=False, stemmer=True)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text)\n",
        "print(sample_text_result)\n",
        "sample_text_result = cleanText(sample_text, lemmatize=True, stemmer=False)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqQi34UoPvq"
      },
      "source": [
        "## Section 1.2: BAG OF WORDS\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in many ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document.\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN8sZqXaNLe7",
        "outputId": "b6bb0b12-926b-4fd1-a722-ae5b71d2760c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "5*12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3YYSpQzIM05l"
      },
      "outputs": [],
      "source": [
        "# Functions to convert document(s) to a list of words, with the option of removing stopwords. Returns document-term matrix.\n",
        "\n",
        "def createBagOfWords(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    bag_of_words_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    bag_of_words_test = vectorizer.transform(clean_test).toarray()\n",
        "    return bag_of_words_train, bag_of_words_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0iCUBqoX82"
      },
      "source": [
        "## Section 1.3: TF-IDF\n",
        "TF-IDF technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.\n",
        "\n",
        "The number of times a term occurs in a document is called its Term frequency (TF).\n",
        "\n",
        " Document frequency is the number of documents in which the word is present.  Inverse DF (IDF) is the inverse of the document frequency which measures the informativeness of term *t*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L6_-DDMQoXEX"
      },
      "outputs": [],
      "source": [
        "def createTFIDF(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = TfidfVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer =  TfidfVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    tfidf_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    tfidf_test = vectorizer.transform(clean_test).toarray()\n",
        "    return tfidf_train, tfidf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0jS45epcC5"
      },
      "source": [
        "# **Section 2: UNDERSTANDING THE DATA : A REVIEWS DATASET**\n",
        "\n",
        "Sentiment analysis is the interpretation and classification of emotions (such as positive, negative and neutral) within text data using text analysis techniques.  \n",
        "Given below is a dataset consisting of reviews along with sentiment class (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "jU6875-NxrHw",
        "outputId": "ed3da1fe-f38a-4e73-a0ef-24d2f3baf64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading reviews.csv...\n",
            "Reviews dataset loaded successfully! Shape: (999, 2)\n"
          ]
        }
      ],
      "source": [
        "# Load the Reviews CSV file directly from the project directory\n",
        "# Make sure you have 'reviews.csv' file in the same folder as this notebook\n",
        "import pandas as pd\n",
        "print(\"Loading reviews.csv...\")\n",
        "df = pd.read_csv('reviews.csv')\n",
        "print(f\"Reviews dataset loaded successfully! Shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HILJMpa_y26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews Dataset Info:\n",
            "Columns: ['sentence', 'sentiment']\n",
            "Shape: (999, 2)\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The rest of the movie lacks art, charm, meanin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence sentiment\n",
              "0  Not sure who was more lost - the flat characte...         0\n",
              "1  Attempting artiness with black & white and cle...         0\n",
              "2         Very little music or anything to speak of.         0\n",
              "3  The best scene in the movie was when Gerardo i...         1\n",
              "4  The rest of the movie lacks art, charm, meanin...         0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display basic info about the reviews dataset\n",
        "print(\"Reviews Dataset Info:\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WXD0iAR5k62v"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MTCScT7uOdUy",
        "outputId": "cff5f17c-ed8e-4488-f5ee-973f907ccd99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The rest of the movie lacks art, charm, meanin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>I just got bored watching Jessice Lange take h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>In a word, it is embarrassing.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Exceptionally bad!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>All in all its an insult to one's intelligence...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>955 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence sentiment\n",
              "0    Not sure who was more lost - the flat characte...         0\n",
              "1    Attempting artiness with black & white and cle...         0\n",
              "2           Very little music or anything to speak of.         0\n",
              "3    The best scene in the movie was when Gerardo i...         1\n",
              "4    The rest of the movie lacks art, charm, meanin...         0\n",
              "..                                                 ...       ...\n",
              "994  I just got bored watching Jessice Lange take h...         0\n",
              "995  Unfortunately, any virtue in this film's produ...         0\n",
              "996                     In a word, it is embarrassing.         0\n",
              "997                                 Exceptionally bad!         0\n",
              "998  All in all its an insult to one's intelligence...         0\n",
              "\n",
              "[955 rows x 2 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7j9rNGkQpn7y"
      },
      "outputs": [],
      "source": [
        "df.to_csv('reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QXo11Bxvytu"
      },
      "source": [
        "# **Section 3: KNN MODEL**\n",
        "\n",
        "Given below are two KNN models; in the first case we are using Bag-of-Words and in the second case we are using TF-IDF.\n",
        "Note the different metrics and parameters used in each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R5yZXboZ92Z4"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 1: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"],\n",
        "                                                        test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,\n",
        "                                         metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    return predicted, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPuI3wrL-8ZJ"
      },
      "source": [
        "Note: Cross-validation will be discussed in detail in the upcoming lab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzno1rRNHoFT",
        "outputId": "8fbd4d27-814e-402f-ef83-ba4c964667da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy = 64.3979057591623%\n",
            "Cross Validation Accuracy: 0.65\n",
            "[0.64313725 0.60392157 0.7007874 ]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\IIIT\\iHub_AIML_2025\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "## KNN accuracy after using BoW\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI5NMP8L-4eW",
        "outputId": "09252b50-b0d7-4914-d327-4f56398d89b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with TFIDF accuracy = 71.72774869109948%\n",
            "Cross Validation Accuracy: 0.73\n",
            "[0.71764706 0.74509804 0.73622047]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\IIIT\\iHub_AIML_2025\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "## KNN accuracy after using TFIDF\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2vYDVfa5AP"
      },
      "source": [
        "# Section 4: SPAM TEXT DATASET\n",
        "Now let's use what we've learnt to classify texts as spam or not spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "c1GZgvXCsKX1",
        "outputId": "fb83f5bd-34f1-408d-fad7-c8150721f04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading spam.csv...\n",
            "Spam dataset loaded successfully! Shape: (5572, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Category                                            Message\n",
              "0         ham  Go until jurong point, crazy.. Available only ...\n",
              "1         ham                      Ok lar... Joking wif u oni...\n",
              "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         ham  U dun say so early hor... U c already then say...\n",
              "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...       ...                                                ...\n",
              "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568      ham              Will Ã¼ b going to esplanade fr home?\n",
              "5569      ham  Pity, * was in mood for that. So...any other s...\n",
              "5570      ham  The guy did some bitching but I acted like i'd...\n",
              "5571      ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the spam text data CSV file directly from the project directory\n",
        "# Make sure you have 'spam.csv' file in the same folder as this notebook\n",
        "# You can download it from: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
        "print(\"Loading spam.csv...\")\n",
        "df_spam = pd.read_csv('spam.csv')\n",
        "print(f\"Spam dataset loaded successfully! Shape: {df_spam.shape}\")\n",
        "df_spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qRiS7dT7piTE",
        "outputId": "f8d38d88-adfa-4dd1-810a-e921e51fbf5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spam Dataset Info:\n",
            "Columns: ['Category', 'Message']\n",
            "Shape: (5572, 2)\n",
            "\n",
            "Sample data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display basic info about the spam dataset  \n",
        "print(\"Spam Dataset Info:\")\n",
        "print(f\"Columns: {list(df_spam.columns)}\")\n",
        "print(f\"Shape: {df_spam.shape}\")\n",
        "print(\"\\nSample data:\")\n",
        "df_spam.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OsvBm4luNCME"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category mapping completed:\n",
            "ham -> 0, spam -> 1\n",
            "\n",
            "Category distribution:\n",
            "Category\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Map text categories to numerical values\n",
        "df_spam['Category'] = df_spam['Category'].map({'ham': 0, 'spam': 1})\n",
        "print(\"Category mapping completed:\")\n",
        "print(\"ham -> 0, spam -> 1\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df_spam['Category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KiEHuMypWyb6",
        "outputId": "a506b452-ea26-4279-cc7b-cff235b3caaa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Category                                            Message\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display first 5 rows of the processed spam dataset\n",
        "df_spam.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRJU9rFy1XQR",
        "outputId": "e2a1c44f-2106-42ab-c463-f83fbb693019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of messages in spam dataset: 5572\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5572"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the total number of records in the spam dataset\n",
        "print(f\"Total number of messages in spam dataset: {len(df_spam)}\")\n",
        "len(df_spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wnv0v4T6sqJQ"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 2: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    return predicted, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8PwydHYs1h_",
        "outputId": "99590f63-44cd-457d-a8f7-5a01fa3b5003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy = 92.19730941704036%\n",
            "Cross Validation Accuracy: 0.91\n",
            "[0.9064603  0.89973082 0.91313131]\n",
            "\n",
            "\n",
            "Cross Validation Accuracy: 0.91\n",
            "[0.9064603  0.89973082 0.91313131]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf8i2P1nxpl8",
        "outputId": "6b2e322f-5454-4f36-fa11-2c6d37c243c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with TFIDF accuracy = 98.56502242152466%\n",
            "Cross Validation Accuracy: 0.97\n",
            "[0.96837147 0.96769852 0.96363636]\n",
            "Cross Validation Accuracy: 0.97\n",
            "[0.96837147 0.96769852 0.96363636]\n"
          ]
        }
      ],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9xvbzS4yLTr"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "2. Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "3. Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6xPL6smyWG-"
      },
      "source": [
        "### Useful Resources for further reading\n",
        "1. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "2. TF-IDF and BoW : https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
        "3. TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uUw16_tkGdV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Answer to Questions**\n",
        "\n",
        "## **Question 1: Why does the TF-IDF approach generally result in better accuracy than Bag-of-Words?**\n",
        "\n",
        "TF-IDF generally outperforms Bag-of-Words because it addresses BoW's key limitations by incorporating word importance weighting. While BoW only counts word occurrences (treating all words equally), TF-IDF uses two critical components:\n",
        "\n",
        "• Term Frequency (TF): Measures how frequently a word appears in a document\n",
        "• Inverse Document Frequency (IDF): Measures how rare/important a word is across the entire corpus\n",
        "\n",
        "This approach reduces the weight of common words (like \"the\", \"is\", \"and\") that appear frequently across documents but carry little meaning, while boosting the weight of rare, discriminative words that are more likely to be topic-specific. In classification tasks, this means TF-IDF creates more meaningful feature vectors that capture the essence of document content rather than just word counts, leading to better separation between different classes and improved accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Document 4: 'The the the is is is and and and' ===\n",
            "\n",
            "BoW weights (raw counts):\n",
            "'and': 3\n",
            "'is': 3\n",
            "'the': 3\n",
            "\n",
            "TF-IDF weights (importance-adjusted):\n",
            "'and': 0.5348\n",
            "'is': 0.5348\n",
            "'the': 0.6542\n",
            "\n",
            "=== Analysis ===\n",
            "BoW gives high weights to common words like 'the', 'is', 'and'\n",
            "TF-IDF reduces these weights significantly because they appear in many documents\n",
            "This makes TF-IDF better at capturing meaningful content differences!\n"
          ]
        }
      ],
      "source": [
        "# Code Example: Demonstrating TF-IDF vs BoW Weight Differences\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample documents for comparison\n",
        "sample_docs = [\n",
        "    \"The movie is very good and entertaining\",\n",
        "    \"The movie is not good and boring\", \n",
        "    \"This film is excellent and amazing\",\n",
        "    \"The the the is is is and and and\"  # Document with many common words\n",
        "]\n",
        "\n",
        "# Create BoW representation\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(sample_docs)\n",
        "bow_feature_names = bow_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create TF-IDF representation  \n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_docs)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Compare weights for the last document (lots of common words)\n",
        "print(\"=== Document 4: 'The the the is is is and and and' ===\")\n",
        "print(\"\\nBoW weights (raw counts):\")\n",
        "doc4_bow = bow_matrix[3].toarray()[0]\n",
        "for i, word in enumerate(bow_feature_names):\n",
        "    if doc4_bow[i] > 0:\n",
        "        print(f\"'{word}': {doc4_bow[i]}\")\n",
        "\n",
        "print(\"\\nTF-IDF weights (importance-adjusted):\")\n",
        "doc4_tfidf = tfidf_matrix[3].toarray()[0]\n",
        "for i, word in enumerate(tfidf_feature_names):\n",
        "    if doc4_tfidf[i] > 0:\n",
        "        print(f\"'{word}': {doc4_tfidf[i]:.4f}\")\n",
        "\n",
        "print(\"\\n=== Analysis ===\")\n",
        "print(\"BoW gives high weights to common words like 'the', 'is', 'and'\")\n",
        "print(\"TF-IDF reduces these weights significantly because they appear in many documents\")\n",
        "print(\"This makes TF-IDF better at capturing meaningful content differences!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Question 2: Can you think of techniques that are better than both BoW and TF-IDF?**\n",
        "\n",
        "Yes, several modern techniques outperform both BoW and TF-IDF by addressing their fundamental limitation: lack of semantic understanding. The most significant improvements come from:\n",
        "\n",
        "• Word Embeddings (Word2Vec, GloVe): Create dense vector representations that capture semantic relationships between words (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\")\n",
        "• Contextualized Embeddings (BERT, GPT, RoBERTa): Generate different representations for the same word based on context (e.g., \"bank\" in \"river bank\" vs \"money bank\")\n",
        "• N-grams with Neural Networks: Capture local word order and phrase meanings that BoW/TF-IDF completely ignore\n",
        "• Doc2Vec/Paragraph Vectors: Learn document-level representations that consider word order and document structure\n",
        "• Transformer-based Models: Use attention mechanisms to understand long-range dependencies and complex linguistic patterns\n",
        "\n",
        "These techniques achieve better results because they understand word relationships, context, and meaning rather than just treating text as isolated word counts or weighted frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Comparing BoW vs N-grams for capturing meaning ===\n",
            "\n",
            "BoW Features: ['all' 'at' 'bad' 'good' 'is' 'movie' 'not' 'the' 'very']\n",
            "BoW Vectors:\n",
            "Sentence 1: [0 0 0 1 1 1 1 1 0]\n",
            "Sentence 2: [0 0 0 1 1 1 0 1 1]\n",
            "Sentence 3: [1 1 1 1 0 1 1 0 0]\n",
            "BoW Similarity between 'not good' and 'very good': 0.800\n",
            "\n",
            "============================================================\n",
            "\n",
            "N-gram Features (1-gram + 2-gram): ['all' 'at' 'at all' 'bad' 'bad at' 'good' 'good movie' 'is' 'is not'\n",
            " 'is very' 'movie' 'movie is' 'movie not' 'not' 'not bad' 'not good' 'the'\n",
            " 'the movie' 'very' 'very good']\n",
            "N-gram Vectors:\n",
            "Sentence 1: [0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0]\n",
            "Sentence 2: [0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1]\n",
            "Sentence 3: [1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0]\n",
            "N-gram Similarity between 'not good' and 'very good': 0.667\n",
            "\n",
            "=== Key Insight ===\n",
            "N-grams capture phrases like 'not good' vs 'very good' as separate features\n",
            "This leads to better understanding of negation and context!\n",
            "Modern techniques like BERT/GPT do this even better with deep learning.\n"
          ]
        }
      ],
      "source": [
        "# Code Example: N-grams (Better than BoW/TF-IDF) - Capturing Word Order\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example sentences with different meanings but similar words\n",
        "sentences = [\n",
        "    \"The movie is not good\",        # Negative sentiment\n",
        "    \"The movie is very good\",       # Positive sentiment  \n",
        "    \"Good movie, not bad at all\"    # Positive sentiment\n",
        "]\n",
        "\n",
        "print(\"=== Comparing BoW vs N-grams for capturing meaning ===\\n\")\n",
        "\n",
        "# 1. Standard BoW (ignores word order)\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(sentences)\n",
        "print(\"BoW Features:\", bow_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Vectors:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {bow_matrix[i].toarray()[0]}\")\n",
        "\n",
        "# Calculate similarity between sentences 1 and 2 using BoW\n",
        "bow_sim = cosine_similarity(bow_matrix[0:1], bow_matrix[1:2])[0][0]\n",
        "print(f\"BoW Similarity between 'not good' and 'very good': {bow_sim:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# 2. N-grams (captures word order and phrases)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Unigrams + Bigrams\n",
        "ngram_matrix = ngram_vectorizer.fit_transform(sentences)\n",
        "print(\"N-gram Features (1-gram + 2-gram):\", ngram_vectorizer.get_feature_names_out())\n",
        "print(\"N-gram Vectors:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {ngram_matrix[i].toarray()[0]}\")\n",
        "\n",
        "# Calculate similarity between sentences 1 and 2 using N-grams\n",
        "ngram_sim = cosine_similarity(ngram_matrix[0:1], ngram_matrix[1:2])[0][0]\n",
        "print(f\"N-gram Similarity between 'not good' and 'very good': {ngram_sim:.3f}\")\n",
        "\n",
        "print(f\"\\n=== Key Insight ===\")\n",
        "print(\"N-grams capture phrases like 'not good' vs 'very good' as separate features\")\n",
        "print(\"This leads to better understanding of negation and context!\")\n",
        "print(\"Modern techniques like BERT/GPT do this even better with deep learning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Question 3: Pros and Cons of Stemming vs Lemmatization**\n",
        "\n",
        "Both stemming and lemmatization aim to reduce words to their root forms, but they differ significantly in approach and outcomes:\n",
        "\n",
        "### Stemming:\n",
        "• Pros: Fast and computationally efficient, works well for IR/search tasks, simple rule-based approach, reduces vocabulary size effectively\n",
        "• Cons: Crude heuristic approach, can produce non-dictionary words (e.g., \"better\" → \"better\"), may over-stem or under-stem, ignores word context and meaning\n",
        "\n",
        "### Lemmatization:  \n",
        "• Pros: Produces actual dictionary words (lemmas), considers word context and part-of-speech, more linguistically accurate, better for semantic analysis\n",
        "• Cons: Computationally expensive, requires full vocabulary and morphological analysis, slower processing, may not improve retrieval performance significantly\n",
        "\n",
        "### Key Insight: \n",
        "Stemming prioritizes speed and recall (finding more matches), while lemmatization prioritizes accuracy and precision (finding correct matches). The choice depends on your specific application needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEMMING vs LEMMATIZATION COMPARISON ===\n",
            "\n",
            "Original        Stemmed         Lemmatized      Notes\n",
            "----------------------------------------------------------------------\n",
            "running         run             run             Same result\n",
            "ran             ran             ran             Same result\n",
            "runs            run             run             Same result\n",
            "easily          easili          easily          Different approaches\n",
            "fairly          fairli          fairly          Different approaches\n",
            "better          better          well            Lemma more accurate\n",
            "good            good            good            Same result\n",
            "dogs            dog             dog             Same result\n",
            "feet            feet            foot            Different approaches\n",
            "geese           gees            geese           Stem more aggressive\n",
            "organizing      organ           organize        Stem more aggressive\n",
            "organized       organ           organize        Stem more aggressive\n",
            "organization    organ           organization    Stem more aggressive\n",
            "am              am              be              Different approaches\n",
            "are             are             be              Different approaches\n",
            "is              is              be              Different approaches\n",
            "was             wa              be              Different approaches\n",
            "were            were            be              Different approaches\n",
            "being           be              be              Same result\n",
            "\n",
            "=== PERFORMANCE COMPARISON ===\n",
            "Stemming time: 0.1497 seconds\n",
            "Lemmatization time: 0.4955 seconds\n",
            "Lemmatization is 3.3x slower than stemming\n",
            "Stemming time: 0.1497 seconds\n",
            "Lemmatization time: 0.4955 seconds\n",
            "Lemmatization is 3.3x slower than stemming\n"
          ]
        }
      ],
      "source": [
        "# Code Example: Stemming vs Lemmatization Comparison\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import time\n",
        "\n",
        "# Sample words to demonstrate differences\n",
        "test_words = [\n",
        "    \"running\", \"ran\", \"runs\", \"easily\", \"fairly\", \n",
        "    \"better\", \"good\", \"dogs\", \"feet\", \"geese\",\n",
        "    \"organizing\", \"organized\", \"organization\",\n",
        "    \"am\", \"are\", \"is\", \"was\", \"were\", \"being\"\n",
        "]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get POS tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print(\"=== STEMMING vs LEMMATIZATION COMPARISON ===\\n\")\n",
        "print(f\"{'Original':<15} {'Stemmed':<15} {'Lemmatized':<15} {'Notes'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Compare each word\n",
        "for word in test_words:\n",
        "    # Stemming (fast, rule-based)\n",
        "    stemmed = stemmer.stem(word)\n",
        "    \n",
        "    # Lemmatization (slower, context-aware)\n",
        "    lemmatized = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    \n",
        "    # Determine if results differ\n",
        "    if stemmed == lemmatized:\n",
        "        note = \"Same result\"\n",
        "    elif stemmed in [\"better\", \"good\"] or lemmatized in [\"better\", \"good\"]:\n",
        "        note = \"Lemma more accurate\"\n",
        "    elif len(stemmed) < len(lemmatized):\n",
        "        note = \"Stem more aggressive\"\n",
        "    else:\n",
        "        note = \"Different approaches\"\n",
        "    \n",
        "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15} {note}\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
        "test_text = \" \".join(test_words * 1000)  # Large text for timing\n",
        "\n",
        "# Time stemming\n",
        "start_time = time.time()\n",
        "stemmed_results = [stemmer.stem(word) for word in word_tokenize(test_text)]\n",
        "stem_time = time.time() - start_time\n",
        "\n",
        "# Time lemmatization  \n",
        "start_time = time.time()\n",
        "lemma_results = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokenize(test_text)]\n",
        "lemma_time = time.time() - start_time\n",
        "\n",
        "print(f\"Stemming time: {stem_time:.4f} seconds\")\n",
        "print(f\"Lemmatization time: {lemma_time:.4f} seconds\") \n",
        "print(f\"Lemmatization is {lemma_time/stem_time:.1f}x slower than stemming\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
